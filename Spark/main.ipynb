{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13af19f3-c977-4217-9512-112fd9c449f9",
   "metadata": {},
   "source": [
    "### Написание приложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "812ff316-83c6-4855-b72d-c9fc7e08ec35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------+----+------------+---------+--------+--------+--------+-----------+------------+\n",
      "|rank|                name|platform|year|       genre|publisher|na_sales|eu_sales|jp_sales|other_sales|global_sales|\n",
      "+----+--------------------+--------+----+------------+---------+--------+--------+--------+-----------+------------+\n",
      "|   1|          Wii Sports|     Wii|2006|      Sports| Nintendo|   41.49|   29.02|    3.77|       8.46|       82.74|\n",
      "|   2|   Super Mario Bros.|     NES|1985|    Platform| Nintendo|   29.08|    3.58|    6.81|       0.77|       40.24|\n",
      "|   3|      Mario Kart Wii|     Wii|2008|      Racing| Nintendo|   15.85|   12.88|    3.79|       3.31|       35.82|\n",
      "|   4|   Wii Sports Resort|     Wii|2009|      Sports| Nintendo|   15.75|   11.01|    3.28|       2.96|        33.0|\n",
      "|   5|Pokemon Red/Pokem...|      GB|1996|Role-Playing| Nintendo|   11.27|    8.89|   10.22|        1.0|       31.37|\n",
      "+----+--------------------+--------+----+------------+---------+--------+--------+--------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MySparkApp\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"video_game_sales.csv\", header=True, inferSchema=True)\n",
    "df.show(5)\n",
    "\n",
    "# Применение трансформаций\n",
    "result_df = df.groupBy(\"publisher\").agg({\"global_sales\": \"sum\"})\n",
    "\n",
    "# Запись результата в CSV\n",
    "# Запись результата обратно в HDFS (или локальную файловую систему) с режимом \"overwrite\"\n",
    "result_df.write.mode(\"overwrite\").csv(\"hdfs://namenode:9000/path/to/output.csv\", header=True)\n",
    "\n",
    "# Завершение работы\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e7da38-9d35-4359-baed-ea0d2cc077ef",
   "metadata": {},
   "source": [
    "### Распределенные набора данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c521dcbd-e324-432b-8f22-8271578f7ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Создание SparkContext\n",
    "sc = SparkContext(\"local\", \"RDD Example\")\n",
    "\n",
    "# Создание RDD из списка\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Применение map для преобразования значений\n",
    "squared_rdd = rdd.map(lambda x: x ** 2)\n",
    "\n",
    "# Получение результатов\n",
    "result = squared_rdd.collect()  # Вернет список\n",
    "print(result)\n",
    "\n",
    "# Остановка SparkContext\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462bba7e-b447-4161-9cf7-784ad83dde96",
   "metadata": {},
   "source": [
    "### Фреймы данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "29bdfaf4-1ecc-4dc3-9cb7-5ffda22d32bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    1|\n",
      "|    2|\n",
      "|    3|\n",
      "|    4|\n",
      "+-----+\n",
      "\n",
      "+--------+----------+\n",
      "|category|sum(value)|\n",
      "+--------+----------+\n",
      "|       A|         4|\n",
      "|       B|         6|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Создание SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrame Example\").getOrCreate()\n",
    "\n",
    "# Пример данных\n",
    "data = [Row(value=1, category='A'),\n",
    "        Row(value=2, category='B'),\n",
    "        Row(value=3, category='A'),\n",
    "        Row(value=4, category='B')]\n",
    "\n",
    "# Создание DataFrame из списка\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# Выбор столбцов\n",
    "df.select(\"value\").show()\n",
    "\n",
    "# Применение агрегирования\n",
    "df.groupBy(\"category\").agg({\"value\": \"sum\"}).show()\n",
    "\n",
    "# Остановка SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29808ba4-47c2-4bcb-a716-03ec52d824db",
   "metadata": {},
   "source": [
    "### Расширение логики работы с данными с помощью UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cdc5ac-d81f-485c-a619-63cb21768f9d",
   "metadata": {},
   "source": [
    "UDF (User-Defined Functions) позволяют вам определять собственные функции для обработки данных в Spark. Они могут быть использованы как в RDD, так и в DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "097090d9-a296-4fe2-9131-522c66ff0a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|value|squared_value|\n",
      "+-----+-------------+\n",
      "|    1|            1|\n",
      "|    2|            4|\n",
      "|    3|            9|\n",
      "|    4|           16|\n",
      "|    5|           25|\n",
      "+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Создание Spark Context и Session\n",
    "sc = SparkContext(\"local\", \"Example\")\n",
    "spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n",
    "\n",
    "# Создание RDD\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Преобразование RDD в DataFrame с использованием Row\n",
    "df = spark.createDataFrame(rdd.map(lambda x: Row(value=x)))\n",
    "\n",
    "# Определение UDF\n",
    "def square(x):\n",
    "    return x * x\n",
    "\n",
    "square_udf = udf(square, IntegerType())\n",
    "\n",
    "# Применение UDF к DataFrame\n",
    "df_with_square = df.withColumn(\"squared_value\", square_udf(df[\"value\"]))\n",
    "\n",
    "# Показать результат\n",
    "df_with_square.show()\n",
    "\n",
    "# Завершение работы\n",
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1fb63e-3643-406a-9fe8-9411657e04ae",
   "metadata": {},
   "source": [
    "### Запись данных в HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ce0e977-b60a-4fac-b9fe-08e9d30bf9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|category|value|\n",
      "+--------+-----+\n",
      "|       A|   10|\n",
      "|       B|   20|\n",
      "|       A|   30|\n",
      "|       B|   40|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Создание SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Hadoop Integration Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Создание данных в виде списка объектов Row\n",
    "data = [\n",
    "    Row(category='A', value=10),\n",
    "    Row(category='B', value=20),\n",
    "    Row(category='A', value=30),\n",
    "    Row(category='B', value=40)\n",
    "]\n",
    "\n",
    "# Преобразование списка в DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()\n",
    "\n",
    "# Применение трансформаций\n",
    "result_df = df.groupBy(\"category\").agg({\"value\": \"sum\"})\n",
    "\n",
    "# Запись результата в HDFS\n",
    "result_df.write.csv(\"hdfs://namenode:9000/user/kseo/dir/output.csv\", header=True)\n",
    "\n",
    "# Завершение работы\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aafb9402-b113-46ac-9c3d-8a9c1bb43ef1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1249926768.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[45], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    Вот простой пример приложения на Python, использующего PySpark для обработки данных:\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#### Пример на Python\n",
    "\n",
    "Вот простой пример приложения на Python, использующего PySpark для обработки данных:\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Создание SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "\n",
    "    .appName(\"MySparkApp\") \\\n",
    "\n",
    "    .getOrCreate()\n",
    "\n",
    "# Чтение данных из CSV файла\n",
    "\n",
    "df = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Показать данные\n",
    "\n",
    "df.show()\n",
    "\n",
    "# Применение трансформаций\n",
    "\n",
    "result_df = df.groupBy(\"category\").agg({\"value\": \"sum\"})\n",
    "\n",
    "# Запись результата в CSV\n",
    "\n",
    "result_df.write.csv(\"output.csv\", header=True)\n",
    "\n",
    "# Завершение работы\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81946802-e610-453b-9172-e46110ffe046",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
